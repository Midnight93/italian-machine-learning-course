{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Introduzione a TensorFlow 2.0 e TensorFlow Datasets",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/galeone/italian-machine-learning-course/blob/master/Introduzione_a_TensorFlow_2_0_e_TensorFlow_Datasets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5qcyzY9Bbcl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x \n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odivAp-OBtuF",
        "colab_type": "text"
      },
      "source": [
        "Google Colab ci fornisce un ambiente pronto all'uso per usare TensorFlow 2.0.\n",
        "Nella cella precedente abbiamo usato il magic-command (comando non Python, ma specifico dei Jupyter Notebook) `%tensorflow_version` per impostare nel runtime corrente l'uso di TensorFlow 2.x.\n",
        "\n",
        "TensorFlow 2.0 è eager by default: ogni riga di codice è eseguita in ordine sequenziale, esattamente così com'è scritta.\n",
        "\n",
        "TensorFlow 1.x, invece, utilzzava un diverso paradigma di programazione. Infatti, TensorFlow era un framework che funzionava in modo *descrittivo*: prima veniva definita la computazione, e solo in seguito eseguita \"all-at-once\".\n",
        "\n",
        "Usare TensorFlow 1.x all'interno di Jupyter Notebook era molto scomodo, ma fortunatamente la nuova versione permette di utilizzare ogni tool creato per Python (come i notebook), essendo TensorFlow 2.0 molto più \"pythonico\".\n",
        "\n",
        "## Obiettivo\n",
        "\n",
        "L'obietivo di questo notebook è risolvere un problema di classificazione su immagini, usando una rete completamente connesssa.\n",
        "\n",
        "Gli step da seguire, propri di ogni buona pipeline di sviluppo di progetti Machine Learning sono:\n",
        "\n",
        "- Ottenere ed Analizzare i dati\n",
        "- Definire la pipeline di input\n",
        "- Definire il modello\n",
        "- Definire le metriche\n",
        "- Definire il training loop\n",
        "- Allenare il modello e misurare le metriche durante ed alla fine di ogni epoca\n",
        "- Selezionare il modello migliore (basandosi sulla metrica di validation)\n",
        "- Misurare le performance sullo split di test\n",
        "\n",
        "Il tutto utilizzando come strumento di data visualization principale **TensorBoard**.\n",
        "\n",
        "## Ottenere ed Analizzare i dati\n",
        "\n",
        "In un caso d'uso reale, avere un dataset di dati etichettati è un processo lungo e noiso (che qualcuno però deve fare); fortunatamente, per sperimentare diversi algoritmi di machine learning, esistono dataset  (solitamente prodotti da universià o industrie) che sono diventati lo standard.\n",
        "\n",
        "TensorFlow, ha deciso di standardizzare e semplificare il processo di ottenimento dei dataset mediante la libreria [**TensorFlow Datasets** (tfds)](https://www.tensorflow.org/datasets/).\n",
        "\n",
        "Anziché dover manualmente scaricare e processare i dati, dai siti delle università/industrie, possiamo usare tfds, per (automaticamente):\n",
        "\n",
        "- scaricare il dataset di dati grezzi\n",
        "- applicare trasformazioni ai dati grezzi in modo tale da renderli usabili\n",
        "- trasformare questi dati in `TFRecord` (formato ottimizzato per i dati usato da TensorFlow)\n",
        "- ottenere un oggetto `tf.data.Dataset` (oggetti per pipeline di input altemete efficienti) pronto all'uso.\n",
        "\n",
        "Essendo una libreria separata, è necessario scaricarla ed installarla nel sistema usando pip:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KlDb8_4UDcsl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! pip install --upgrade tensorflow_datasets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wokDCe4YK7xH",
        "colab_type": "text"
      },
      "source": [
        "Siamo ora pronti per conoscere TensorFlow datasets.\n",
        "\n",
        "La libreria è molto semplice e pratica da usare: tutto si basa sul concetto di Dataset Builder. Un dataset builder è una classe (implementata all'interno di tfds) che contiene tutto il processo logico per scaricare, trasfromare, ed ottenere il dataset sotto forma di oggetto `tf.data.Dataset`.\n",
        "\n",
        "Vedere la lista dei builders (e quindi dei dataset) disponibili è semplice:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GUJ7cjPKjYN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow_datasets as tfds\n",
        "print(tfds.list_builders())\n",
        "print(len(tfds.list_builders()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7tunfZ-RbVB",
        "colab_type": "text"
      },
      "source": [
        "I dataset disponibili sono ~100. Per ognuno di questi è disponibile una descrizione completa sul [catalogo online](https://www.tensorflow.org/datasets/catalog/overview).\n",
        "\n",
        "Per sperimentare i nostri modelli di classificazione, scegliamo di utilizzare il dataset `\"cifar10\"`.\n",
        "\n",
        "Questo dataset è un dataset tipicamente utilizzato per fare benchmark di algoritmi di computer vision ed è composto da immagini a colori 32x32.\n",
        "\n",
        "TensorFlow datasets ci offre, mediante il metodo load, si ottenere sia il dataset che le **informazioni** relative al tipo di dati che questo contiene."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xz7NBrHLnRt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data, info = tfds.load(\"cifar10\", with_info=True, split=tfds.Split.ALL)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BEU7C5vVSkKW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(info)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqsDDNP5T8em",
        "colab_type": "text"
      },
      "source": [
        "Grazie all'oggetto `DatasetInfo` abbiamo già una prima analisi del dataset:\n",
        "\n",
        "Il dataset viene fornito direttamente con degli split:\n",
        "\n",
        "- ci sono 50000 immagini di train\n",
        "- ci sono 10000 immagini di test\n",
        "- **non c'è un validation set** (dovremmo crearlo noi)\n",
        "- le immagini sono `32 x 32 x 3` ed il loro tipo è `tf.uint8` (il ché implica valori in [0,255])\n",
        "- le label sono 10 ed il tipo è `tf.int64` (uno scalare, non codificato one-hot)\n",
        "\n",
        "Leggendo l'[API reference di TensorFlow Datasets](https://www.tensorflow.org/datasets/api_docs/python/tfds) è possibile trovare diverse funzioni messe a nostra disposizione per poter visualizzare ed analizzare il dataset.\n",
        "\n",
        "Una delle più interessanti [`tfds.show_examples`](https://www.tensorflow.org/datasets/api_docs/python/tfds/show_examples) che dato un dataset e le sue informazioni, ci permette di visualizzare direttamente in un notebook (in quanto ritorna un oggetto matplotlib) alcuni samples dal dataset:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjcMGbIHTI_T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig = tfds.show_examples(info, data, rows=4, cols=4)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKdhsIN6XTVa",
        "colab_type": "text"
      },
      "source": [
        "Come visibile dall'immagine, all'interno del datataset è presente la coppia immagine label, ed all'interno dell'oggetto info, invece, è presente la relazione che lega la label testuale alla label numerica.\n",
        "\n",
        "Dall'API documentation è possibile trovare i metodi [str2int](https://www.tensorflow.org/datasets/api_docs/python/tfds/features/ClassLabel#str2int) ed [int2str](https://www.tensorflow.org/datasets/api_docs/python/tfds/features/ClassLabel#int2str) che permettono di passare da stringa a label e viceversa.\n",
        "\n",
        "Essendo TensorFlow 2.0 eager by default, possiamo iniziare ad utilizzarlo per creare un loop su oggetti di tipo `tf.Tensor`, prodotti all'operazione `tf.range` (equivalente alla `range` di Python). Nel loop visualizziamo la relazione tra label numerica e stringa:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9359Pb-bVZQk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for label in tf.range(10):\n",
        "  print(label.numpy(), \" -> \", info.features[\"label\"].int2str(label))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B--amRprdkay",
        "colab_type": "text"
      },
      "source": [
        "## Dataset API: definire la pipeline di input\n",
        "\n",
        "`data` è un oggetto di tipo `tf.data.Dataset`: la dataset API è **ottimizzata** per creare pipeline di input per il train di modelli.\n",
        "\n",
        "L'API è basata sul **method chaining**: i metodi dell'oggetto dataset, applicano trasformazioni al dataset corrente, e ritornano un dataset con la trasofmrazione applicata.\n",
        "\n",
        "La dataset API rappresenta correttamente il processo di ETL (Extract-Transform-Load) tipici di una pipeline di data science.\n",
        "\n",
        "- TensorFlow Datasets è incaricato dell'estrazione dei dati e della prima trasformazione\n",
        "- tf.data.Dataset con i suoi metodi applica la serie di trasformazioni atte a rendere utili i dati\n",
        "- L'iterazione sull'oggetto dataset è il load dei dati in memoria\n",
        "\n",
        "![etl](https://i.imgur.com/YRCqeAO.png)\n",
        "\n",
        "La pipeline di trasformazioni che vogliamo applicare è questa:\n",
        "\n",
        "- trasformare i dati da uint a float\n",
        "- codificare one-hot le label\n",
        "- scalare le immagini nel range [-1,1]\n",
        "- \"appiattire\" (flatten) le immagini, per renderle anziché tensori `32 x 32 x 3`, dei tensori `32*32*3`\n",
        "- Creare gli split di train, validation, test (tre oggetti `tf.data.Dataset`)\n",
        "- Per ognuno di questi: creare dei batch di dimensione 32 per poter fare, successivamente, mini-batch gradient descent / valutazione in batch\n",
        "- ottimizzare le performance della pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltmyq97VZc3V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def transform(row):\n",
        "  # trasformare i dati da uint a float\n",
        "  row[\"image\"] = tf.image.convert_image_dtype(row[\"image\"], dtype=tf.float32)\n",
        "  # 1-hot\n",
        "  row[\"label\"] = tf.one_hot(row[\"label\"], depth=10, on_value=1, off_value=0)\n",
        "  # [-1,1] range\n",
        "  row[\"image\"] = (row[\"image\"] - 0.5) * 2.\n",
        "  # flatten\n",
        "  row[\"image\"] = tf.reshape(row[\"image\"], (-1,))\n",
        "  return row\n",
        "\n",
        "dataset = data.map(transform)\n",
        "\n",
        "# split, batch, prefetch\n",
        "train = dataset.take(50000).batch(32).prefetch(1)\n",
        "validation = dataset.skip(50000).take(5000).batch(32).prefetch(1)\n",
        "test = dataset.skip(50000 + 5000).take(5000).batch(32).prefetch(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0W9cY6QsRCu",
        "colab_type": "text"
      },
      "source": [
        "## Definizione del modello: Keras API\n",
        "\n",
        "Keras è un API specification per la definizione ed il training di modelli di machine learning che TensorFlow ha deciso di adottare.\n",
        "\n",
        "L'API è molto intuitiva da usare e si trova all'interno del modulo [`tf.keras`](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/).\n",
        "\n",
        "Keras offre tre differenti modi per creare un modello:\n",
        "\n",
        "- Sequential API\n",
        "- Functional API\n",
        "- Subclassing\n",
        "\n",
        "I layer offerti da Keras sono i più disparati: dai layer dense, ai layer di attivazione, ai layer di batch normalization, a quelli di convoluzioni per lavorare su immagini o pointcloud e molti altri.\n",
        "\n",
        "Ogni layer è una classe da poter istanziare e configurare tramite i parametri del costruttore.\n",
        "\n",
        "Per esempio, il costruttore del layer `tf.keras.layers.Dense` ha la seguente firma:\n",
        "\n",
        "```python\n",
        "__init__(\n",
        "    units,\n",
        "    activation=None,\n",
        "    use_bias=True,\n",
        "    kernel_initializer='glorot_uniform',\n",
        "    bias_initializer='zeros',\n",
        "    kernel_regularizer=None,\n",
        "    bias_regularizer=None,\n",
        "    activity_regularizer=None,\n",
        "    kernel_constraint=None,\n",
        "    bias_constraint=None,\n",
        "    **kwargs\n",
        ")\n",
        "```\n",
        "\n",
        "Nella quale possiamo specificare *ogni cosa* relativa all funzionamento del layer: dal numero di unità (neuroni), all'uso o meno del bias, al tipo di inizializzazione dei parametri.\n",
        "\n",
        "Dato che la definizione di un modello è completamente arbitraria, possiamo provare a partire con un semplice modello (pochi neuroni per layer, per evitare l'overfitting), che riduce la dimensionalità dell'input layer per layer, fino ad arrivare a 10 neuroni di output (le classi).\n",
        "\n",
        "Un semplice modello fully connected può essere visto come uno stack di layer `Dense`, ed è il caso d'uso canonico della Sequential API.\n",
        "\n",
        "## Sequential API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6R_ekD8BdyyM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "  tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
        "  tf.keras.layers.Dense(256, activation=tf.nn.relu),\n",
        "  tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
        "  tf.keras.layers.Dense(10) # Note the linear activation\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qsV69zES_uS",
        "colab_type": "text"
      },
      "source": [
        "Keras permette opzionalmente, di specificare la dimensione dell'input in fase di definizione del modello, oppure di lasciare che sia il Keras model alla prima esecuzione a determinarla in maniera automatica.\n",
        "\n",
        "Questo può sembrare un particolare da poco, ma in realtà è qualcosa di fondamentale.\n",
        "\n",
        "Conoscere a priori la dimensione dell'input permette di definire il **grafo computazionale** del modello completamente, e quindi poter visualizzare il \"riassunto\" completo del modello.\n",
        "\n",
        "Ogni keras model offre il metodo `.summary()` per ottenere una visualizzazione tablellare della struttura del modello, ma per ottenre il numero dei parametri del primo layer, è **sempre** necessario conoscere la dimensione di input.\n",
        "\n",
        "Infatti, per poter completamente definire la *matrice* dei pesi del primo layer, è necessario non solo il numero di neuroni, ma anche il numero di dimensioni dell'input.\n",
        "\n",
        "Difatti, se proviamo a invocare il metodo `.summary()` sul modello appena creato, otteniamo il seguente errore:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OE7LXwBUvKJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATdo0HPtUxWe",
        "colab_type": "text"
      },
      "source": [
        "Per poter visualizzare il summary completo, non avendo definito come attributo `input_shape` del primo layer, dobbiamo effettuare un **forward pass** del modello con un tensore di input (della dimensione coretta), in modo tale che Keras possa costrure il grafo computazionale."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scUH4OK1Uwfh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fake_input = tf.zeros((1, 32*32*3))\n",
        "out = model(fake_input)\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53JaDxgR1hBi",
        "colab_type": "text"
      },
      "source": [
        "Il numero di parametri è stato correttamente calcolato (e come è possibile vedere, per un modello così semplice siamo già oltre il milione di parametri), sebbene l'`output shape` risulti \"multiple\", anziché essere del valore corretto.\n",
        "\n",
        "La **raccomadazione** è  di specificare **sempre** in fase di creazione del modello la dimensione dell'input, un modo tale da poter ottenere summary rappresentativi ed aiutare Keras nella definizione del modello stesso.\n",
        "\n",
        "Possiamo quindi sovrascrivere il modelo precedente, creandolo ex-novo, ma specificando l'input shape. Per specificarla abbiamo due modi:\n",
        "\n",
        "- O usare un `tf.keras.layers.Input` layer\n",
        "- O usara il parametro del costruttore del primo layer dense `input_shape`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBE-MrAKVHU7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "  tf.keras.layers.Input(shape=(32*32*3)),\n",
        "  tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
        "  tf.keras.layers.Dense(256, activation=tf.nn.relu),\n",
        "  tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
        "  tf.keras.layers.Dense(10) # Note the linear activation\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEnIcVX-2hJG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xt3qN2Sa439n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.keras.utils.plot_model(model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9eJCt3q5bX_",
        "colab_type": "text"
      },
      "source": [
        "## Functional API\n",
        "\n",
        "Un modo differente per definire i modelli, è quello di usare la functional API.\n",
        "\n",
        "Ogni layer Keras è un oggetto **callable**: questo significa che è possibile utilizzare un oggetto istanziato come se fosse una funzione, che accetta un input e produce un output.\n",
        "\n",
        "Per un modello con un singolo input ed un singolo output, totalmente sequenziale (come il nostro) non è necessario utlizzarla, i quanto Sequential soddisfa pienamente ogni requisito.\n",
        "\n",
        "In ogni modo, essendo l'API più flessibile offerta da Keras per definire modelli, è bene conoscerla ed utlizzarla il più possibile, in modo tale da essere familiari con questa API quando si definiranno modelli con più input, più output e con relazioni tra i layer del modello."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKOJ-qJC5HYH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs = tf.keras.layers.Input(shape=(32*32*3))\n",
        "net = tf.keras.layers.Dense(512, activation=tf.nn.relu)(inputs)\n",
        "net = tf.keras.layers.Dense(256, activation=tf.nn.relu)(net)\n",
        "net = tf.keras.layers.Dense(128, activation=tf.nn.relu)(net)\n",
        "out = tf.keras.layers.Dense(10)(net)\n",
        "model = tf.keras.Model(inputs=inputs, outputs=out)\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcsKI8VL8AN-",
        "colab_type": "text"
      },
      "source": [
        "## Loss function\n",
        "\n",
        "All'interno del modulo [`tf.keras.losses`](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/losses) troviamo una lunga lista di loss function pronte all'uso.\n",
        "\n",
        "Le loss disponibili sono tra le più disparate e scegliere quella corretta dipende da:\n",
        "\n",
        "- il problema che stiamo risolvendo (classificazione, regressione, ...)\n",
        "- il formato delle nostre label\n",
        "\n",
        "Dato che:\n",
        "\n",
        "- abbiamo codificato in one-hot le label\n",
        "- **non** abbiamo applicato una non-linearità al layer di outut del modello (volutamente)\n",
        "\n",
        "La nostra scelta deve ricadere sulle funzioni: **non** sparse e che accettano (unscaled) **logits** come input.\n",
        "\n",
        "TensorFlow, per le loss, utilizza le keywords **sparse** e **logits** per indicare se la loss function accetta label scalari (non one-hot, ed applica la loss function stessa la conversione all'interno) e output di modelli **lineari**.\n",
        "\n",
        "Quando la loss function accetta label scalari, allora è la loss function stessa che al suo interno applica la conversione a rappresentazione one-hot.\n",
        "\n",
        "Quando la loss function accetta (o permette di specificare) `from_logits=True` significa che sarà la loss function stessa a applicare la non linearità corretta all'output del modello per il calcolo della loss.\n",
        "\n",
        "Ad esempio, per un problema di classificazione multi-classe, con label rappresentate in one-hot, la loss function che viene utilzzata è la **categorical cross-entropy loss**.\n",
        "\n",
        "Quello che vogliamo, è allenare la rete neurale per **produrre una probabilità su C classi** (10 in questo caso) data un immagine di input.\n",
        "\n",
        "La loss calcola la **softmax activation** sull'output della rete (per riscalare i valori di output nel range probabilistico [0,1]) e dopo calcola la cross-entropy-loss.\n",
        "\n",
        "**Softmax**\n",
        "\n",
        "Softmax è una funzione di attivazione che riscala i valori di output di un classificatore nel range [0,1], in modo tale che la somma di tutti i valori predetti sia 1.\n",
        "\n",
        "La softmax activation viene applicata agli **score** predetti dalla rete *s*; dato che gli elementi predetti rappresentano delle classi, questi score possono essere interpretati come probabilità (predizione aereoplano con probabilità 0.8, macchina con probabilità 0.1, ecc).\n",
        "\n",
        "Per una data classe $s_i$, la funzione softmax viene calcolata come\n",
        "\n",
        "$$ f(s)_i = \\frac{e^{s_i}}{\\sum_{j}^{C}{e^{s_j} }} $$\n",
        "\n",
        "**Cross entropy loss**\n",
        "\n",
        "La formula della categorical cross-entropy loss è data dall'applicazione della cross-entropy tra le label $t_i$ (one-hot) e le predizioni dopo il softmax.\n",
        "\n",
        "$$ CE = - \\sum_{i}^{C}{ t_i log(f(s)_i) } $$\n",
        "\n",
        "Dato che la label sono codificate in one hot, solo il componente del vettore dove il valore è 1 concorre al calcolo della loss, mentre tutti gli altri valgono zero.\n",
        "\n",
        "Dato il target vector (label reale), codificato in one hot $t$ e la sua componente ad uno nella posizione $p$ (quindi $t_p$), abbiamo che la formulazione della cross-entropy diventa:\n",
        "\n",
        "$$ CE = -log\\left(\\frac{e^{s_p}}{\\sum_{j}^{C}{e^{s_j}}}\\right) $$\n",
        "\n",
        "### Implementazione\n",
        "\n",
        "Keras si occupa di realizzare **tutto** il calcolo della loss nella maniera più ottimizzata possibile e numericalmente stabile.\n",
        "\n",
        "Come è facile notare, dato che è la loss stessa ad applicare la funzione di attivazione (**softmax**) all'output della rete, quando abbiamo definito il modello abbiamo evitato di aggiungerla all'output layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdIsvd6C7eH9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loss is a callable object\n",
        "loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJy7dleJCnVY",
        "colab_type": "text"
      },
      "source": [
        "Abbiamo il modello `model`, abbiamo il dataset di train `train`, abbiamo la loss function da usare `loss`, ciò che rimane è la definizione del training loop, con annesse scelta dell'ottimizzatore e delle metriche da misurare.\n",
        "\n",
        "## Training loop\n",
        "\n",
        "TensorFlow 2.0 offre una maniera \"avanzata\" per definire ed implementare training loop.\n",
        "\n",
        "Keras, d'altro canto, offre la sua maniera. In questo corso **non tratteremo** il modo Keras di definire ed eseguire i training loop, in quanto **nascondono troppi dettagli** e sono poco flessibili.\n",
        "\n",
        "L'implementazione di un \"custom training loop\" è considerata avanzata, ma in realtà non è nulla di complesso. Anzi, avere controllo sulla fase di forward pass e di calcolo ed applicazione dei gradienti è utile per apprendere in maniera migliore il processo di training ed è senza dubbio l'opzione più flessibile.\n",
        "\n",
        "### Metriche\n",
        "\n",
        "Prima di definire il training loop, è bene scegliere che metriche misurare per tenere monitorate le performance ed identificare eventuali condizioni patologiche.\n",
        "\n",
        "Dato che il dataset è bilanciato, possiamo misurare **l'accuracy**\n",
        "Inoltre, possiamo misurare il valore della loss medio (sul batch) durante il training loop.\n",
        "\n",
        "TensorFlow offre metriche per misurare il valore medio di un qualsiasi scalare che varia nel tempo (`tf.keras.metrics.Mean`), oppure per il calcolo del valore medio di una specifica metrica (e.g. `tf.keras.metrics.Accuracy`).\n",
        "\n",
        "Ogni metrica implementa l'interfaccia standard di Keras relative alle metriche: questo ci garantisce che ogni oggeto appartenente al modulo `tf.keras.metrics` abbia i metodi:\n",
        "\n",
        "- `update_state` (identico al metodo `__call__`): per computare la metrica\n",
        "- `result` per ottenere il valore della metrica computato fin'ora\n",
        "- `reset_state` per resettare lo stato della metrica al valore iniziale"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FmbvgWLChd3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "accuracy = tf.keras.metrics.Accuracy()\n",
        "mean_loss = tf.keras.metrics.Mean(name=\"loss\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGJwZTHwcceS",
        "colab_type": "text"
      },
      "source": [
        "## Loggare le metriche\n",
        "\n",
        "Per loggare le metriche abbiamo due opzioni, da implementare **sempre** assieme quando si definisce una pipeline di ML (ben definita):\n",
        "\n",
        "- loggare su stanard output/error\n",
        "- loggare su **TensorBoard**\n",
        "\n",
        "TensorBoard è un programma che viene installato assieme al modulo tensorflow e permette di visualizzare su grafici curve, istogrammi, dataset embedding, immagini, tracce audio, e molto altro.\n",
        "\n",
        "È perfettamente integrato con i Jupyter notebook e tramite il magic command `%tensorboard` è possibile lansciare un'istanza del tensorboard server.\n",
        "\n",
        "TensorBoard necessita di una cartella da \"monitorare\": all'interno di questa cartella vanno inseriti tutti i dati da loggare (summary).\n",
        "\n",
        "TensorFlow, tramite il modulo `tf.summary` da la possibilità di salvare su file i valori delle metriche, delle immagin utilizzate e di ogni altro tipo di dato che il modulo `tf.summary` supporta.\n",
        "\n",
        "Il concetto fondamentale per poter correttamente utilizzare i summary è quello di `FileWriter`.\n",
        "\n",
        "Questo oggetto permette di creare un **contesto** e tramite questo, scrivere i dati all'interno della cartella monitorata da tensorboard.\n",
        "Il **contesto** è fondamentale per una **buona organizzazione dei log**; infatti, è possibile creare un contesto di train, uno di validation ed uno di test, e visualizzare sullo stesso plot curve (in colori differenti) relative a contesti differenti.\n",
        "\n",
        "Ad esempio, dato un `FileWriter` `writer`, possiamo definire un contesto tramite a keyword python `with` e scrivere all'interno del contesto creato dal writer, in questo modo:\n",
        "\n",
        "```python\n",
        "with writer.as_default():\n",
        "  for step in range(100):\n",
        "    # other model code would go here\n",
        "    tf.summary.scalar(\"my_metric\", 0.5, step=step)\n",
        "    writer.flush()\n",
        "```\n",
        "\n",
        "Per il nostro caso, possiamo creare tre writer differenti"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Du81dKeCcbkZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_writer = tf.summary.create_file_writer(\"logs/train\")\n",
        "validation_writer = tf.summary.create_file_writer(\"logs/validation\")\n",
        "test_writer = tf.summary.create_file_writer(\"logs/test\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uY_E3csxvh6",
        "colab_type": "text"
      },
      "source": [
        "Avendo definito le metriche ed i file writer, siamo pronti a definire il training loop.\n",
        "\n",
        "## Training loop\n",
        "\n",
        "Il loop di training consiste in due parti. Durante ogni epoca di training, dobbiamo calcolare il valore della loss sul batch, **tenere traccia** delle operazioni effettuate durante il calcolo.\n",
        "\n",
        "Tenere traccia delle operazioni fatte è di fondamentale importanza, in quanto possiamo utilizzare queste informazioni per calcolare **il gradiente**, quindi stimare la direzione dell'aggioranamento, ed usare un **ottimizzatore** per applicare l'update dei parametri nella direzione stimata.\n",
        "\n",
        "TensorFlow 2.0 ci aiuta nella modularizzazione del codice: essendo eager by default, possiamo scrivere funzioni Python che effettuano determinate operazioni e mediante **tf.function** è possibile anche accelerare il calcolo di alcune di queste, convertendo il codice in una rappresentazione a grafo altamente ottimizzata."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zV4zud5gqBCc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define the optimizer\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)\n",
        "\n",
        "def compute_loss(input_samples):\n",
        "    predictions = model(input_samples[\"image\"])\n",
        "    loss_value = loss(input_samples[\"label\"], predictions)\n",
        "    return loss_value\n",
        "\n",
        "@tf.function\n",
        "def train_step(input_samples):\n",
        "  with tf.GradientTape() as tape:\n",
        "    loss_value = compute_loss(input_samples)\n",
        "\n",
        "  gradient = tape.gradient(loss_value, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradient, model.trainable_variables))\n",
        "  return loss_value\n",
        "\n",
        "def measure_metrics(input_samples):\n",
        "  predicted_labels = tf.argmax(model(input_samples[\"image\"]), axis=1)\n",
        "  accuracy.update_state(tf.argmax(input_samples[\"label\"], axis=1), predicted_labels)\n",
        "  mean_loss.update_state(compute_loss(input_samples))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EksCULOa-__E",
        "colab_type": "text"
      },
      "source": [
        "Dopo aver definito i \"macroblocchi\" del nostro train, possiamo definire direttamente il training loop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Nf07TQy-_Ct",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "global_step = tf.Variable(0, dtype=tf.int64, trainable=False)\n",
        "epoch_counter = tf.Variable(0, dtype=tf.int64, trainable=False)\n",
        "\n",
        "def train_loop(num_epochs):\n",
        "  \n",
        "  for epoch in tf.range(epoch_counter, num_epochs):\n",
        "    for input_samples in train:\n",
        "      loss_value = train_step(input_samples)\n",
        "      measure_metrics(input_samples)\n",
        "      global_step.assign_add(1)\n",
        "\n",
        "      if tf.equal(tf.math.mod(global_step, 100), 0):\n",
        "        mean_loss_value = mean_loss.result() \n",
        "        accuracy_value = accuracy.result()\n",
        "        mean_loss.reset_states()\n",
        "        accuracy.reset_states()\n",
        "        tf.print(f\"[{global_step.numpy()}] loss value: \", mean_loss_value,\" - train acc: \", accuracy_value)\n",
        "        with train_writer.as_default():\n",
        "          tf.summary.scalar(\"loss\", mean_loss_value, step=global_step)\n",
        "          tf.summary.scalar(\"accuracy\", accuracy_value, step=global_step)\n",
        "          tf.summary.image(\"images\", tf.reshape(input_samples[\"image\"], (-1, 32,32,3)), step=global_step, max_outputs=5)\n",
        "    # end of epoch: measure performance on validation set and log the values on tensorboard\n",
        "    tf.print(f\"Epoch {epoch.numpy() + 1 } completed\")\n",
        "    epoch_counter.assign(epoch + 1)\n",
        "    # TODO: insert validation code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svUBzxG7B7Io",
        "colab_type": "text"
      },
      "source": [
        "Dopo aver definito la funzione di train, con annessa misura delle performance di train e validation, possiamo lanciare tensorboard e subito dopo invocare la funzione di train."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efDU90l0B5NQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir logs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ry0wTMidCGem",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loop(num_epochs=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_BqJD_px14K",
        "colab_type": "text"
      },
      "source": [
        "## Quali sono i problemi di questo training loop?\n",
        "\n",
        "- Non stiamo misurando le performance di validation durante il training [**esercizio 1**]\n",
        "- Al termine del train non stiamo misurando le performance sul test set [**esercizio 2**]\n",
        "- Non c'è persistenza del modello: se il train deovesse interrompersi per qualsiasi motivo (fallimento hardware e simili) dovremmo re-iniziare il training dall'inizio, in quanto **non abbiamo salvato lo stato del modello**. [sezione successiva]\n",
        "- C'è un problema con la visualizzazione delle immagini in TensorBoard: la funzione `tf.summary.image` si aspetta immagini scalate i [0,1] mentre le nostre immagini sono state riscalate in [-1,1] durante la `map_fn`. [**esercizio 3**]\n",
        "- Non effettuiamo alcuna model selection: non gestendo la persistenza del modello e non misurando le performance sul validation set, non abbiamo tenuto monitorato l'overfitting (quando le performance di train sono troppo migliori delle performance di validation) [**esercizio 4**]\n",
        "\n",
        "Alcuni di questi punti sono facilmente risolvibili con le conoscenze acquisite fin'ora, ma per gestire la persistenza è necessario introdurre il concetto di **training checkpoints**.\n",
        "\n",
        "## Training Checkpoints\n",
        "\n",
        "Salvare lo stato di un modello in TensorFlow 2.0 è davvero facile: tutto ciò che è necessario è craere un oggetto checkpoint ed assegnargli (direttamente nel costruttore) gli oggetti che vogliamo salvare.\n",
        "\n",
        "Molti oggetti TensorFLow 2.0 sono \"checkpointable\", il ché significa che sono salvabili su disco dall'oggetto checkpoint (aka sono serializzabili).\n",
        "\n",
        "Per usare un checkpoint è necessario un `CheckpointManager` che permette di gestirli.\n",
        "\n",
        "esempio:\n",
        "\n",
        "```python\n",
        "ckpt = tf.train.Checkpoint(step=tf.Variable(1), optimizer=opt, net=net)\n",
        "manager = tf.train.CheckpointManager(ckpt, './tf_ckpts', max_to_keep=3)\n",
        "ckpt.restore(manager.latest_checkpoint)\n",
        "if manager.latest_checkpoint:\n",
        "  print(\"Restored from {}\".format(manager.latest_checkpoint))\n",
        "else:\n",
        "  print(\"Initializing from scratch.\")\n",
        "```\n",
        "\n",
        "Nel nostro esempio, vogliamo salvare lo stato del modello, dell'ottimizatore (anche se non stiamo usando un ottimizzatore che definisce variabili, ma è lo stesso una buona pratica) e il global step, così da poter riprendere il train dallo step esatto in cui è stato interrotto."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXnvkivWCOfB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ckpt = tf.train.Checkpoint(step=global_step, optimizer=optimizer, model=model)\n",
        "manager = tf.train.CheckpointManager(ckpt, 'ckpts', max_to_keep=3)\n",
        "ckpt.restore(manager.latest_checkpoint)\n",
        "if manager.latest_checkpoint:\n",
        "  tf.print(f\"Restored from {manager.latest_checkpoint}\")\n",
        "else:\n",
        "  tf.print(\"Initializing from scratch.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmcKV28feimP",
        "colab_type": "text"
      },
      "source": [
        "Avendo associato un CheckpointManager ad un oggetto `Checkpoint`, possiamo usarlo per salvare/ripristinare lo stato del modello.\n",
        "\n",
        "Il manager altro non fa' che creare un'associazione tra l'oggetto `Checkpoint` **ed una cartella** (`ckpts`) dove verranno salvati gli oggetti \"attaccati\" al checkpoint.\n",
        "\n",
        "Il metodo da utilizzare per salvare lo stato corrente è il metodo `.save()` del manager:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "koZKhm5f0JWV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "manager.save()\n",
        "ckpt.restore(manager.latest_checkpoint)\n",
        "if manager.latest_checkpoint:\n",
        "  tf.print(f\"Restored from {manager.latest_checkpoint}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFdzxH5qfsSF",
        "colab_type": "text"
      },
      "source": [
        "## Esercizio 1\n",
        "\n",
        "Crea una nuova funzione `training_loop_v2` che aggiunga le misure delle performance sul validation set alla fine di ogni epoca. Usare il `FileWriter` corretto e gestire correttamente lo stato delle metriche.\n",
        "Scrivere i risultati su tensorboard e verificare se una nuova curva appare sullo stesso grafico del training.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KotsNqqxe3rd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "global_step = tf.Variable(0, dtype=tf.int64)\n",
        "\n",
        "def train_loop(num_epochs):\n",
        "  \n",
        "  for epoch in tf.range(num_epochs):\n",
        "    for input_samples in train:\n",
        "      loss_value = train_step(input_samples)\n",
        "      measure_metrics(input_samples)\n",
        "      global_step.assign_add(1)\n",
        "\n",
        "      if tf.equal(tf.math.mod(global_step, 100), 0):\n",
        "        mean_loss_value = mean_loss.result() \n",
        "        accuracy_value = accuracy.result()\n",
        "        mean_loss.reset_states()\n",
        "        accuracy.reset_states()\n",
        "        tf.print(f\"[{global_step.numpy()}] loss value: \", mean_loss_value,\" - train acc: \", accuracy_value)\n",
        "        with train_writer.as_default():\n",
        "          tf.summary.scalar(\"loss\", mean_loss_value, step=global_step)\n",
        "          tf.summary.scalar(\"accuracy\", accuracy_value, step=global_step)\n",
        "          tf.summary.image(\"images\", tf.reshape(input_samples[\"image\"], (-1, 32,32,3)), step=global_step, max_outputs=5)\n",
        "    # end of epoch: measure performance on validation set and log the values on tensorboard\n",
        "    tf.print(f\"Epoch {epoch.numpy() + 1 } completed\")\n",
        "    # TODO: insert validation code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMJCRpgpgK7j",
        "colab_type": "text"
      },
      "source": [
        "## Esercizio 2\n",
        "\n",
        "Definire una funzione `test()` che misuri le performance sul test set ed invocarla al seguito dell'esecuzione della funzione `training_loop_v2` eseguita per 10 epoche."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBges1cLgJnJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test():\n",
        "  #TODO\n",
        "\n",
        "training_loop_v2(num_epochs=10)\n",
        "test()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWhJh7nbgbHi",
        "colab_type": "text"
      },
      "source": [
        "## Esercizio 3\n",
        "\n",
        "Creare una funzione che scali un tensore a valori in [0,1] in un tensore a valori in [-1,1] (codice già presente nel notebook).\n",
        "Creare una seconda funzione che scali un tensore a valori in [-1,1] in [0,1]: usare `tf.summary.image` con immagini scalate nel range corretto ([0,1]): aggiornare ed eseguire le funzioni di training loop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IkP5OCR0gKRY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rescale(image):\n",
        "  return (image + 1.) / 2.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aoM4fI8hAb8",
        "colab_type": "text"
      },
      "source": [
        "## Esercizio 4\n",
        "\n",
        "La funzione `training_loop_v2` misura correttamente l'accuracy sul training set e sul validation set.\n",
        "\n",
        "Modificare il codice della funzione per:\n",
        "\n",
        "- ripristinare lo stato del modello dall'ultimo training step raggiungo prima di iniziare il training loop sulle nuove epoche richieste (usare un checkpoint ed un checkpoint manager per salvare il modello al termine di ogni epoca)\n",
        "- dopo aver misurato l'accuracy di validation e l'accuracy di train (al termine di ogni epoca), usare un **diverso** checkpoint e checkpoint managert (su una diversa folder) per salvare il modello che ha raggiunto la miglior validation accuracy.\n",
        "- Se per 2 epoche consecutive, le performance di train sono migliori delle performance di validation interrompere il training (early stopping basato sul confronto delle metriche)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fl48dFjCg3aJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUzbNq2ZiCTE",
        "colab_type": "text"
      },
      "source": [
        "## Esercizio 5\n",
        "\n",
        "Creare un nuovo notebook per risolvere lo stesso problema, ma:\n",
        "\n",
        "- non usare la funzione di one-hot encoding per le label, ma delegare l'encoding alla Keras loss adeguata: modificare ogni parte del codice necessaria ad usare le label scalari, anziché la rappresentazione one-hot.\n",
        "\n",
        "## Esercizio 6\n",
        "\n",
        "Sperimentare!\n",
        "\n",
        "- Provare come variano le performance al variare del learning rate\n",
        "- Cambiare dataset, scegliendo tra altri presenti in TensorFlow datasets (adeguare quindi il modello)\n",
        "- Sperimentare come variano le performance al variare dell'ottimizzatore (vedesi lista degli ottimizzatori [nella documentazione](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/optimizers))\n",
        "- Variare l'architettura del modello, cambiare numero di neuroni, numero di layer, funzioni di attivazione, struttura, ...\n",
        "\n",
        "## Conclusione\n",
        "\n",
        "La soluzione proposta per questo problema di classificazione di immagini è subottimale: stiamo usando una archietttura fully connected, con milioni di parametri, quando esiste una soluzione ben più efficiente, con un numero minore di parametri e con performance migliori: le reti neurali convoluzionali.\n",
        "\n",
        "Questi tipo di rete ha rivoluzionato il campo della computer vision e del machine learning in generale: tutt'oggi sono i blocchi fondamentali per la stragrande maggioranza delle architetture che lavorano su immagini, audio (e anche per i modelli generativi!).\n",
        "\n",
        "Nel prossimo notebook introdurremo l'operazione di convoluzione, le reti neurali convoluzionali, definiremo un'architettura deep usando stack di layer convoluzionali e risolveremo non solo il problema della classificazione, ma apprenderemo anche come **localizzare** un oggetto all'interno di una immagine, regredendo le coordinate della bounding box."
      ]
    }
  ]
}