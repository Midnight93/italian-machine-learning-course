{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Object Detection and Classification + TensorFlow Hub",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/galeone/italian-machine-learning-course/blob/master/Object_Detection_and_Classification_%2B_TensorFlow_Hub.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ra4CPI3J6mLj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x \n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUai7jFJ7GNY",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "## Object detection and classification + Transfer Learning\n",
        "\n",
        "Definire una rete neurale convouzionale con \"due teste\" (double headed convolutional neural network) in modo tale da risolvere il problema di classificazione e localizzazione di un oggetto all'interno dell'immagine.\n",
        "\n",
        "È necessario definire la rete con due distinte \"teste\" in quanto:\n",
        "\n",
        "- la regression head, avrà l'unico scopo di regredire le coordinate dell'oggetto presente all'interno dell'immagine\n",
        "- la classification head, classificherà l'immagine.\n",
        "\n",
        "Per poter implementare questo tipo di rete, siamo vincolati ad usare la **functional** API di Keras, in quanto la topologia non è un semplice stack di layer, ma si biforca nella parte finale.\n",
        "\n",
        "L'architettura è formata da:\n",
        "\n",
        "- una parte di feature extraction, che altro non è che uno stack di layer convoluzionali, incaricati di estrarre feature utili per i layer di regressione e classificazione\n",
        "- le due teste che, usando le stesse feature estratte dal feature extractor, apprenderanno la capacità di classificare e regredire le coordinate della boundingbox (rispettivamente).\n",
        "\n",
        "Per implmenetare il feature extractor abbiamo due opzioni:\n",
        "\n",
        "1. Scrivere il feature extractor e ri-allenarlo da zero\n",
        "2. Utilizzare un modello pre-trainato\n",
        "\n",
        "Li vedremo entrambi.\n",
        "\n",
        "## Preparazione del dataset (subset PASCAL VOC 2012)\n",
        "\n",
        "Esistono diversi dataset di object detection; un'immagine contiene 1+ oggetti annotati, con annotazione delle 4 coordinate + la classe.\n",
        "\n",
        "Dato che siamo interessati a risolvere il problema dell'object localization + classifcation, dobbiamo estrarre un sottoinsieme delle immagini di un dataset di object detection, cercando solo le immagini che contengono una (ed una sola) bounding box per immagine.\n",
        "\n",
        "Il dataset che andremo ad utilizzare è il dataset **PASCAL VOC 2012**: questo dataset è un dataset usato per fare benchmark di algoritmi di image classifcation, object detection, semantic segmentation e human pose estimation.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DGr_ZYq7BxL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install --upgrade tensorflow_datasets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAbjjVzb7NtZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow_datasets as tfds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MC6W7BCX7UW6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get the dataset (splits are OK)\n",
        "# Train, test, and validation are datasets for object detection: multiple objects per image. \n",
        "(train, test, validation), info = tfds.load(\n",
        "    \"voc2007\", split=[\"train\", \"test\", \"validation\"], with_info=True\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-xnZ_ZI7S5e",
        "colab_type": "text"
      },
      "source": [
        "Ottenuto il dataset, dobbiamo scrivere:\n",
        "\n",
        "1. Una funzione che disegni sull'immagine la bouning box dell'oggetto trovato\n",
        "2. La funzione che filtri il dataset ed estragga solo le immagini con una singola annotazione\n",
        "\n",
        "Per poter capire come lavorare sul dataset, sfruttiamo l'oggetto `info`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjLcGgj67PI4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(info)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8pTpDF77c7k",
        "colab_type": "text"
      },
      "source": [
        "È facile notare che per ogni immagine è presenta una `Sequence` di oggetti, con varie informazioni a disposizioni, tra le quali le 4 coordinate (assolute, valori tra [0,1]), la label, ed altre informazioni sull'oggetto.\n",
        "\n",
        "Per avere un'idea del contenuto dataset, utilizziamo la funzione `tfds.show_examples` e dopo definitiamo la funzione filtro che utilizzeremo per estrarre il sottoinsieme da noi desiedrato per ogni split."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzxLWuJw7YmC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig = tfds.show_examples(info, train, rows=4, cols=4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dAs_EJZ7mim",
        "colab_type": "text"
      },
      "source": [
        "Differentemente dai dataset per la classificazione, qui non vediamo le bounding boxes disegnate attorni agli oggetti ne tantomeno la label associata all'immagine (perché solitamente ce ne è più di una).\n",
        "\n",
        "Possiamo passare ora alla definizione della funzione filtro ed alla sua applicazione agli split, seguita dalla definizione della funzione di disegno **di una singola bounding box** sull'immagine."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPFZLVPz7gLF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a subset of the dataset by filtering the elements: we are interested \n",
        "# in creating a dataset for object detetion and classification that is a dataset \n",
        "# of images with a single object annotated. \n",
        "def filter(dataset): \n",
        "    return dataset.filter(lambda row: tf.equal(tf.shape(row[\"objects\"][\"label\"])[0], 1)) \n",
        " \n",
        "train, test, validation = filter(train), filter(test), filter(validation)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1IDKl7py7pgj",
        "colab_type": "text"
      },
      "source": [
        "## Creazione della pipeline di input\n",
        "\n",
        "Il dataset filtrato è pronto, dobbiamo solo costrire la pipeline di input utilizzando il method chaining e costruire i batch.\n",
        "\n",
        "Dato che le immagini sono tutte di dimensione differente, dobbiamo scalarle ad una risoluzione uguale in modo da poter creare dei batch.\n",
        "\n",
        "La risoluzione scelta è `(299,299)` - la scelta della risoluzione non è casuale, ma è quella che si aspetta il feature extractor pre-trainato che utilizzeremo in seguito (**inception-v3**)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SAMpO0AO7iks",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare(row):\n",
        "  # Scaling in [0,1] because inception v3 expects input values in this range\n",
        "  row[\"image\"] = tf.image.convert_image_dtype(row[\"image\"], tf.float32) \n",
        "  row[\"image\"] = tf.image.resize(row[\"image\"], (299, 299))\n",
        "  return row\n",
        "\n",
        "train = train.map(prepare).batch(32).prefetch(1)\n",
        "validation = validation.map(prepare).batch(32).prefetch(1)\n",
        "test = test.map(prepare).batch(32).prefetch(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMMuzRS59y61",
        "colab_type": "text"
      },
      "source": [
        "Possiamo anche definire ed utilizzare un funzione di comodo, che date le immagini e le bounding box corrispondenti, le disegni.\n",
        "\n",
        "L'API di TensorFlow offre già la funzione `tf.image.draw_bounding_boxes` da utilizare, noi ci limitiamo ad utilizzarla gestendo,però, i possibili casi di input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrxU0AXi7rhG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def draw(images, boxes, color=tf.constant((1.0, 1.0, 0, 0))):\n",
        "  images = tf.image.convert_image_dtype(images, tf.float32)\n",
        "  if tf.equal(tf.rank(images), 3):\n",
        "    print(\"expanded dims\")\n",
        "    images = tf.expand_dims(images, axis=[0])\n",
        "  if tf.equal(tf.rank(boxes), 2):\n",
        "    boxes = tf.expand_dims(boxes, axis=[1])\n",
        "  # draw bounding boxes wants [batch_size, num boxes, coordinates]\n",
        "  images = tf.image.draw_bounding_boxes(\n",
        "      images=images,\n",
        "      boxes=boxes,\n",
        "      colors=tf.reshape(color, (1, 4)))\n",
        "  return images"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6hIOSMb72Ei",
        "colab_type": "text"
      },
      "source": [
        "Vista l'integrazione con matplotlib dei jupyter notebook, possiamo provare a visualizzare direttamente qui qualche immagine con la relativa bounding box."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqitMLOM7s7r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# take only a batch and loop over it\n",
        "for row in train.take(1):\n",
        "  images = draw(row[\"image\"], row[\"objects\"][\"bbox\"])\n",
        "  for idx, image in enumerate(images):\n",
        "    tf.print(\"label: \", info.features[\"objects\"][\"label\"].int2str(row[\"objects\"][\"label\"][idx][0])) \n",
        "    plt.imshow(image) \n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihHpqjFo76Cf",
        "colab_type": "text"
      },
      "source": [
        "## Feature-extractor from scratch\n",
        "\n",
        "Definire l'architettura bifronte è facile usando le keras API.\n",
        "\n",
        "Una particolarità dei Keras model, è la loro possibilità di essere utilizzati come layer all'interno di altri modelli più complessi.\n",
        "Possiamo quindi definire una rete formata da tre blocchi:\n",
        "\n",
        "- feature extractor: trasforma l'immagine in in layer di feature a bassa dimensionalità\n",
        "- classification head: un classificatore in grado di classificare l'input nelle n classi supportate\n",
        "- regression head: possiamo definirlo in due modi diversi; o class agnostic (agnostico alla classe) e quidi un regressore di 4 coordinate; oppure class specific, in cui la testa è formata da `4*<numero classi>` neuroni di output.\n",
        "\n",
        "Definire la regression head class agnostic è più facile (ed è efficace quasi in egual modo), quindi definiremo la testa in questo modo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EtXZ31_yF6PD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Feature extractor layer\n",
        "inputs = tf.keras.layers.Input(shape=(299,299,3))\n",
        "net = tf.keras.layers.Conv2D(32, (3,3),strides=(2,2),padding='same',activation=tf.nn.relu)(inputs)\n",
        "net = tf.keras.layers.Conv2D(64, (3,3),strides=(2,2),padding='same',activation=tf.nn.relu)(net)\n",
        "net = tf.keras.layers.Conv2D(128, (3,3),strides=(2,2),padding='same',activation=tf.nn.relu)(net)\n",
        "net = tf.keras.layers.Conv2D(256, (3,3),strides=(2,2),padding='same',activation=tf.nn.relu)(net)\n",
        "net = tf.keras.layers.Conv2D(512, (3,3),strides=(2,2),padding='same',activation=tf.nn.relu)(net)\n",
        "net = tf.keras.layers.Conv2D(1024, (3,3),strides=(2,2),padding='same',activation=tf.nn.relu)(net)\n",
        "features = tf.keras.layers.Flatten()(net)\n",
        "\n",
        "# Regression head\n",
        "net = tf.keras.layers.Dense(512)(features)\n",
        "net = tf.keras.layers.ReLU()(net)\n",
        "coordinates = tf.keras.layers.Dense(4, use_bias=False)(net)\n",
        "\n",
        "# Classification head\n",
        "net = tf.keras.layers.Dense(1024, activation=tf.nn.relu)(features)\n",
        "net = tf.keras.layers.Dense(512, activation=tf.nn.relu)(net)\n",
        "classes = tf.keras.layers.Dense(20)(net)\n",
        "chimera = tf.keras.Model(inputs=inputs, outputs=[classes, coordinates])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tazj0BOArQzj",
        "colab_type": "text"
      },
      "source": [
        "## Loss function e training loop\n",
        "\n",
        "Allenare in parallelelo una rete neurale per due task differenti è il compito del **multi-task learning**.\n",
        "\n",
        "Nella pratica, trattasi di definire una loss function che tenga in considerazione entrambi i problemi da risolvere.\n",
        "\n",
        "In questi casi, la loss diventa una loss pesata formata da due termini:\n",
        "\n",
        "- un termine di classificazione\n",
        "- un termine di regressione delle coordinate\n",
        "\n",
        "$$ \\mathcal{L} = \\lambda_1 \\mathcal{L}_{c} + \\lambda_2 \\mathcal{L}_{r} $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sLoO-9Qr2Fs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def l2(y_true, y_pred):\n",
        "  return tf.reduce_mean(tf.reduce_sum(tf.square(tf.squeeze(y_true, axis=[1]) - y_pred), axis=[1]))\n",
        "\n",
        "regression_loss = l2\n",
        "classification_loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "lambda_1 = 1\n",
        "lambda_2 = 1\n",
        "\n",
        "def loss(y_true, y_pred, box_true, box_pred):\n",
        "  return lambda_1 * classification_loss(y_true, y_pred) + lambda_2 * regression_loss(box_true, box_pred)\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)\n",
        "\n",
        "# common name\n",
        "model = chimera\n",
        "\n",
        "# Metrics (TOOD)\n",
        "mean_loss = tf.keras.metrics.Mean(name=\"loss\")\n",
        "\n",
        "# writers\n",
        "train_writer = tf.summary.create_file_writer(\"odlogs/train\")\n",
        "validation_writer = tf.summary.create_file_writer(\"odlogs/validation\")\n",
        "test_writer = tf.summary.create_file_writer(\"odlogs/test\")\n",
        "\n",
        "def compute_loss(input_samples):\n",
        "    predictions, coordinates = chimera(input_samples[\"image\"])\n",
        "    loss_value = loss(\n",
        "        input_samples[\"objects\"][\"label\"], predictions,\n",
        "        input_samples[\"objects\"][\"bbox\"], coordinates)\n",
        "    return loss_value\n",
        "\n",
        "@tf.function\n",
        "def train_step(input_samples):\n",
        "  with tf.GradientTape() as tape:\n",
        "    loss_value = compute_loss(input_samples)\n",
        "\n",
        "  gradient = tape.gradient(loss_value, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradient, model.trainable_variables))\n",
        "  return loss_value\n",
        "\n",
        "def measure_metrics(input_samples):\n",
        "  # TODO: accuracy metric on bale\n",
        "  # TODO: IoU over predictions\n",
        "  mean_loss.update_state(compute_loss(input_samples))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_f-06-R8I2m",
        "colab_type": "text"
      },
      "source": [
        "## Logging: TensorBoard\n",
        "\n",
        "Dato che TensorBoard è lo strumento offerto da TensorFLow per il logging delle metriche e dei dati, lo utilizziamo al posto dei notebook per la data visualization durante la fase di training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArLRvzxaqKOP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir odlogs/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZflib1S8TSh",
        "colab_type": "text"
      },
      "source": [
        "## Definizione del training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1acYGF29v16V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "global_step = tf.Variable(0, dtype=tf.int64, trainable=False)\n",
        "epoch_counter = tf.Variable(0, dtype=tf.int64, trainable=False)\n",
        "\n",
        "def train_loop(num_epochs):\n",
        "  \n",
        "  for epoch in tf.range(epoch_counter, num_epochs):\n",
        "    for input_samples in train:\n",
        "      loss_value = train_step(input_samples)\n",
        "      measure_metrics(input_samples)\n",
        "      global_step.assign_add(1)\n",
        "\n",
        "      if tf.equal(tf.math.mod(global_step, 10), 0):\n",
        "        mean_loss_value = mean_loss.result() \n",
        "        # TODO metrics\n",
        "        mean_loss.reset_states()\n",
        "        tf.print(f\"[{global_step.numpy()}] loss value: \", mean_loss_value)\n",
        "        with train_writer.as_default():\n",
        "          tf.summary.scalar(\"loss\", mean_loss_value, step=global_step)\n",
        "          # Draw ground truth (yellow)\n",
        "          logme =  draw(input_samples[\"image\"],\n",
        "                                input_samples[\"objects\"][\"bbox\"],\n",
        "                                color=tf.constant((1.0, 0, 0, 0)))\n",
        "          # Draw prediction (red)\n",
        "          predicted_classes, predicted_boxes = model(input_samples[\"image\"])\n",
        "          logme = draw(logme, predicted_boxes)\n",
        "          tf.summary.image(\"gt_vs_prediction\", logme, step=global_step, max_outputs=5)\n",
        "    # end of epoch: measure performance on validation set and log the values on tensorboard\n",
        "    tf.print(f\"Epoch {epoch.numpy() + 1 } completed\")\n",
        "    epoch_counter.assign(epoch + 1)\n",
        "    # TODO: insert validation code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFhFVyPep_kE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loop(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwFb6h8jXhRy",
        "colab_type": "text"
      },
      "source": [
        "## Pretrained feature extractor: TensorFlow Hub\n",
        "\n",
        "TensorFlow Hub è un \"hub\" in cui è possibile trovare modelli pre-allenati printi all'uso. I modelli presenti sono perfettamente integrati con la Keras API e TensorFlow 2.0.\n",
        "\n",
        "La lista completa dei modelli è disponibile sul sito uffuciale: https://tfhub.dev/\n",
        "\n",
        "Essendo l'ecosistema TensorFlow modulare, TensorFlow Hub è un pacchetto Python installabile singolarmente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QeG8lzQYCjp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! pip install --upgrade tensorflow_hub"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ezhlPnAYFF8",
        "colab_type": "text"
      },
      "source": [
        "L'integrazione con Keras è straordinaria: un  solo metodo permette di **scaricare**, **salvare**, utilizzare sia come feature extractor fisso che ri-allenare il modello.\n",
        "\n",
        "Il modello scelto è **Inception V3**.\n",
        "\n",
        "![inc](https://i.imgur.com/HpFoGP0.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hyn8x4ZYEZp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow_hub as hub\n",
        "import os\n",
        "\n",
        "os.environ[\"TFHUB_DOWNLOAD_PROGRESS\"] = \"1\"\n",
        "\n",
        "# Feature extractor layer\n",
        "inputs = tf.keras.layers.Input(shape=(299,299,3))\n",
        "features = hub.KerasLayer(\n",
        "        \"https://tfhub.dev/google/tf2-preview/inception_v3/feature_vector/2\",\n",
        "        output_shape=[2048],\n",
        "        trainable=False,\n",
        "      )(inputs)\n",
        "\n",
        "# Regression head\n",
        "net = tf.keras.layers.Dense(512)(features)\n",
        "net = tf.keras.layers.ReLU()(net)\n",
        "coordinates = tf.keras.layers.Dense(4, use_bias=False)(net)\n",
        "\n",
        "# Classification head\n",
        "net = tf.keras.layers.Dense(1024, activation=tf.nn.relu)(features)\n",
        "net = tf.keras.layers.Dense(512, activation=tf.nn.relu)(net)\n",
        "classes = tf.keras.layers.Dense(20)(net)\n",
        "chimera = tf.keras.Model(inputs=inputs, outputs=[classes, coordinates])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88piUGlN9RrX",
        "colab_type": "text"
      },
      "source": [
        "## Esercizio 1\n",
        "\n",
        "Misurare la classification accuracy nel training loop \"feature extraction from scratch\".\n",
        "\n",
        "## Esercizio 2\n",
        "\n",
        "Misurare le performance di validation al termine di ogni epoca nel training loop \"feature extraction from scratch\".\n",
        "\n",
        "## Esercizio 3\n",
        "\n",
        "Implementare il salvataggio ed il restore dello stato del training loop, mediante i checkpoint.\n",
        "\n",
        "## Esercizio 4\n",
        "\n",
        "Implementare il training loop completo per il modello definito usando il pre-trained feature extractor.\n",
        "\n",
        "## Esercizio 5\n",
        "\n",
        "Provare ad allenare il modello che usa Inception v3 sia facendo transfer learning che facendo fine tuning.\n",
        "Misurare la diferenza di tempo di esecuzione mediante usando `from time import time()`\n",
        "\n",
        "## Esercizio 6\n",
        "\n",
        "Data la funzione per il calcolo dell'intersection over union:\n",
        "\n",
        "```python\n",
        "\n",
        "def iou(pred_box, gt_box, h, w):\n",
        "    \"\"\"\n",
        "    Compute IoU between detect box and gt boxes\n",
        "    Args:\n",
        "        pred_box: shape (4, ):  y_min, x_min, y_max, x_max - predicted box\n",
        "        gt_boxes: shape (n, 4): y_min, x_min, y_max, x_max - ground truth\n",
        "        h: image height\n",
        "        w: image width\n",
        "    \"\"\"\n",
        "\n",
        "    # Transpose the coordinates from y_min, x_min, y_max, x_max\n",
        "    # In absolute coordinates to x_min, y_min, x_max, y_max\n",
        "    # in pixel coordinates\n",
        "    def _swap(box):\n",
        "        return tf.stack([box[1] * w, box[0] * h, box[3] * w, box[2] * h])\n",
        "\n",
        "    pred_box = _swap(pred_box)\n",
        "    gt_box = _swap(gt_box)\n",
        "\n",
        "    box_area = (pred_box[2] - pred_box[0]) * (pred_box[3] - pred_box[1])\n",
        "    area = (gt_box[2] - gt_box[0]) * (gt_box[3] - gt_box[1])\n",
        "    xx1 = tf.maximum(pred_box[0], gt_box[0])\n",
        "    yy1 = tf.maximum(pred_box[1], gt_box[1])\n",
        "    xx2 = tf.minimum(pred_box[2], gt_box[2])\n",
        "    yy2 = tf.minimum(pred_box[3], gt_box[3])\n",
        "\n",
        "    # compute the width and height of the bounding box\n",
        "    w = tf.maximum(0, xx2 - xx1)\n",
        "    h = tf.maximum(0, yy2 - yy1)\n",
        "\n",
        "    inter = w * h\n",
        "    return inter / (box_area + area - inter)\n",
        "````\n",
        "\n",
        "e la soglia di detection pari di IoU >= 0.75, misurare la `tf.keras.metric.Precision` durante il training ed in validation.\n",
        "\n",
        "Interrrompere il train quando il modello ha raggiunto un valore circa costante (+/- un valore epsilon arbitrario) sia per il valore di train IoU che di train accuracy."
      ]
    }
  ]
}